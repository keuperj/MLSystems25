{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKLajLqUni6H"
      },
      "source": [
        "## Getting Started with PyTorch on Cloud TPUs\n",
        "\n",
        "This notebook will show you how to:\n",
        "\n",
        "* Install PyTorch/XLA on Colab, which lets you use PyTorch with TPUs.\n",
        "* Run basic PyTorch functions on TPUs, like creating and adding tensors.\n",
        "* Run PyTorch modules and autograd on TPUs.\n",
        "* Run PyTorch networks on TPUs.\n",
        "\n",
        "PyTorch/XLA is a package that lets PyTorch connect to Cloud TPUs and use TPU cores as devices. Colab provides a free Cloud TPU system (a remote CPU host + four TPU chips with two cores each) and installing PyTorch/XLA only takes a couple minutes.\n",
        "\n",
        "To use PyTorch on Cloud TPUs in your own Colab notebook you can copy this one, or copy the setup cell below and configure your Colab environment to use TPUs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3rCVMRazoeB"
      },
      "source": [
        "<h3>  &nbsp;&nbsp;Use Colab Cloud TPU&nbsp;&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a></h3>\n",
        "\n",
        "* On the main menu, click Runtime and select **Change runtime type**. Set \"TPU\" as the hardware accelerator.\n",
        "* The cell below makes sure you have access to a TPU on Colab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls3j-EWI2D2v"
      },
      "source": [
        "## Creating and Manipulating Tensors on TPUs\n",
        "\n",
        "PyTorch uses Cloud TPUs just like it uses CPU or CUDA devices, as the next few cells will show. Each core of a Cloud TPU is treated as a different PyTorch  device.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "42avAvSg17by"
      },
      "outputs": [],
      "source": [
        "# imports pytorch\n",
        "import torch\n",
        "\n",
        "# imports the torch_xla package\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b8RfPPk4VIX"
      },
      "source": [
        "As mentioned above, the PyTorch/XLA package (torch_xla) lets PyTorch use TPU devices. The `xla_device()` function returns the TPU's \"default\" core as a device. This lets PyTorch creates tensors on TPUs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9KYz-Vk4fMa",
        "outputId": "3c9379d5-6c4e-480a-e848-271cf7c62296"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]], device='xla:0')\n"
          ]
        }
      ],
      "source": [
        "# Creates a random tensor on xla:1 (a Cloud TPU core)\n",
        "dev = xm.xla_device()\n",
        "t1 = torch.ones(3, 3, device = dev)\n",
        "print(t1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHwOC-xr4_LX"
      },
      "source": [
        "See the documentation at http://pytorch.org/xla/ for a description of all public PyTorch/XLA functions. Here `xm.xla_device()` acquired the first Cloud TPU core ('xla:1'). Other cores can be directly acquired, too:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ef7flq95OxD",
        "outputId": "4221a495-ca47-4036-e976-eb3e62902262"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]], device='xla:2')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_xla/core/xla_model.py:99: UserWarning: `devkind` argument is deprecated and will be removed in a future release.\n",
            "  warnings.warn(\"`devkind` argument is deprecated and will be removed in a \"\n"
          ]
        }
      ],
      "source": [
        "# Creating a tensor on the second Cloud TPU core\n",
        "second_dev = xm.xla_device(n=2, devkind='TPU')\n",
        "t2 = torch.zeros(3, 3, device = second_dev)\n",
        "print(t2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANcDKzGG5_ua"
      },
      "source": [
        "It is recommended that you use functions like `xm.xla_device()` over directly specifying TPU cores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiKmkAkoO06x"
      },
      "source": [
        "Tensors on TPUs can be manipulated like any other PyTorch tensor. The following cell adds, multiplies, and matrix multiplies two tensors on a TPU core:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l50-R2kwFY7Z",
        "outputId": "b8d303bf-33aa-45c6-ddde-cee9478fcffb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-2.1709,  0.8687],\n",
            "        [-0.4169, -2.6102]], device='xla:0')\n",
            "tensor([[-1.9041,  1.3752],\n",
            "        [ 0.1211, -1.3700]], device='xla:0')\n",
            "tensor([[ 1.1726, -0.9614],\n",
            "        [ 0.3379,  0.9861]], device='xla:0')\n"
          ]
        }
      ],
      "source": [
        "a = torch.randn(2, 2, device = dev)\n",
        "b = torch.randn(2, 2, device = dev)\n",
        "print(a + b)\n",
        "print(b * 2)\n",
        "print(torch.matmul(a, b))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfDBbDtuisdu"
      },
      "source": [
        "This next cell runs a 1D convolution on a TPU core:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aryiLyezisEg",
        "outputId": "adfbd7ad-18c3-4670-8005-1a680a80bc39"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ -8.3487,  -1.7244,  -0.0633,  ...,   4.4209,   1.7392,  11.0566],\n",
              "         [  0.4492,  -2.5616,   8.3175,  ...,  -6.2764,   4.4541,   2.6496],\n",
              "         [  7.5034,   1.0899,   3.5591,  ...,  -2.7368,  -5.9956,  -0.6176],\n",
              "         ...,\n",
              "         [ -5.6083,  -2.3524,   5.7621,  ...,  -7.1722,   3.0126,   2.2364],\n",
              "         [  5.4591,   2.3915,  -2.6300,  ...,   2.5127,  -6.0706,   6.0664],\n",
              "         [ -6.6130,   0.4147,  13.6904,  ...,  12.4280,   2.1681,   4.4126]],\n",
              "\n",
              "        [[  3.6752,  -3.8668,   2.8230,  ...,   3.9862,  -8.1474,  12.9051],\n",
              "         [  9.4716,  -3.6923,  -3.3784,  ...,  -1.3280,  -3.7977,  10.1738],\n",
              "         [ -3.4809,  -8.7523,  -6.3204,  ..., -10.3281,   6.3623,  10.3201],\n",
              "         ...,\n",
              "         [  3.7361,   3.5268,  -1.5678,  ...,   1.1855,  -0.7647,  -5.9642],\n",
              "         [  4.2686,  -4.6781,   2.3134,  ...,   4.3414,  -5.6396,   7.7923],\n",
              "         [ -1.2181,   6.3962,   4.0468,  ...,   2.6808,  -2.9050,   5.0465]],\n",
              "\n",
              "        [[ 18.8890,  -1.9383,   5.9604,  ..., -12.4677,  -4.5758,  -7.2808],\n",
              "         [ -6.5446,   0.4276,   4.1922,  ...,  -3.4656,  12.5544,   8.2839],\n",
              "         [  5.1938,   7.5171,   3.7206,  ...,  -4.7660,  17.0933,   0.8079],\n",
              "         ...,\n",
              "         [ -2.8182,  -4.6647,   1.8498,  ..., -12.0606, -18.9709,  -4.8042],\n",
              "         [  4.1219, -17.5952,   5.8284,  ...,   9.1285,   0.2359,  -8.2566],\n",
              "         [ -1.6535,   4.0919,  -5.3761,  ...,   1.7155,  -5.9019,   3.6503]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 13.4576,   0.7443, -11.2741,  ...,   7.8177,   1.0743,  -0.3000],\n",
              "         [ 16.3322,   3.6448,   9.6962,  ...,  -1.5166,   8.8319,  -5.4094],\n",
              "         [ -1.6506,  -7.5800,   2.0531,  ...,   0.8664, -11.9418,   6.8694],\n",
              "         ...,\n",
              "         [ -7.5386,  -1.1286,   1.0957,  ...,  -7.2647,   9.6245,  -3.4824],\n",
              "         [ 16.6113, -12.4371,   0.4078,  ...,   6.4097,   2.9870,  -1.1643],\n",
              "         [  3.3692,  -5.5358,   7.3227,  ...,  -0.8783,  13.7721, -12.1831]],\n",
              "\n",
              "        [[-10.1746,  -0.3483,  -7.8377,  ...,  13.9852,   1.3777,   5.0671],\n",
              "         [ 14.2639,  -3.9745,  -1.8158,  ...,   0.6355,   1.4128, -18.4374],\n",
              "         [ 10.4336,   4.2800,   9.6279,  ...,  -0.5747,  -1.2629,  -8.1635],\n",
              "         ...,\n",
              "         [ -1.6215,   1.3181, -13.0455,  ...,  11.6321,   1.1843,   0.2627],\n",
              "         [  5.0527, -12.9563,   9.5953,  ...,  -8.4052,  10.3368,   5.2308],\n",
              "         [ -2.9684,   2.9895,  -3.6589,  ...,   9.2287,  -4.5036,   0.8643]],\n",
              "\n",
              "        [[ -1.2355,  -5.1546,   5.9913,  ..., -11.9949,   1.2444,   2.4457],\n",
              "         [ -3.8001,   9.5183,   3.8910,  ..., -11.3356,  -0.0981,   3.8280],\n",
              "         [-15.1822,  -6.7306,   4.9868,  ...,  -1.4683,  10.6144,  -5.6586],\n",
              "         ...,\n",
              "         [ -1.8751,   7.0754,  -0.0715,  ...,  -7.5322, -12.3158,   9.0108],\n",
              "         [ -7.8078,   6.4962,   2.2412,  ...,  -3.8551,   1.4852,   4.0254],\n",
              "         [ -9.9912,  -0.1910,  -7.2717,  ..., -12.1974,  -4.7533,  11.6888]]],\n",
              "       device='xla:0')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Creates random filters and inputs to a 1D convolution\n",
        "filters = torch.randn(33, 16, 3, device = dev)\n",
        "inputs = torch.randn(20, 16, 50, device = dev)\n",
        "torch.nn.functional.conv1d(inputs, filters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5b4AmYDgbKd"
      },
      "source": [
        "And tensors can be transferred between CPU and TPU. In the following cell, a tensor on the CPU is copied to a TPU core, and then copied back to the CPU again. Note that PyTorch makes copies of tensors when transferring them across devices, so `t_cpu` and `t_cpu_again` are different tensors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WTsdQ3yO-8G",
        "outputId": "d9c45c47-e41c-4bc4-cdfd-232c96206be4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.1314, -0.5279],\n",
            "        [-1.0666, -0.3110]])\n",
            "tensor([[-1.1314, -0.5279],\n",
            "        [-1.0666, -0.3110]], device='xla:0')\n",
            "tensor([[-1.1314, -0.5279],\n",
            "        [-1.0666, -0.3110]])\n"
          ]
        }
      ],
      "source": [
        "# Creates a tensor on the CPU (device='cpu' is unnecessary and only added for clarity)\n",
        "t_cpu = torch.randn(2, 2, device='cpu')\n",
        "print(t_cpu)\n",
        "\n",
        "t_tpu = t_cpu.to(dev)\n",
        "print(t_tpu)\n",
        "\n",
        "t_cpu_again = t_tpu.to('cpu')\n",
        "print(t_cpu_again)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWtOgDLxN_BV"
      },
      "source": [
        "## Running PyTorch modules and autograd on TPUs\n",
        "\n",
        "Modules and autograd are fundamental PyTorch components.\n",
        "\n",
        "In PyTorch, every stateful function is a module. Modules are Python classes augmented with metadata that lets PyTorch understand how to use them in a neural network. For example, linear layers are modules, as are entire networks. Since modules are stateful, they can be placed on devices, too. PyTorch/XLA lets us place them on TPU cores:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-WT-r8sRERM",
        "outputId": "10ca36ba-c870-4018-d14e-26b2e0962e65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0295, -0.1017],\n",
            "        [-0.3432,  0.0148],\n",
            "        [ 0.9068,  0.6823]], device='xla:0', grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Creates a linear module\n",
        "fc = torch.nn.Linear(5, 2, bias=True)\n",
        "\n",
        "# Copies the module to the XLA device (the first Cloud TPU core)\n",
        "fc = fc.to(dev)\n",
        "\n",
        "# Creates a random feature tensor\n",
        "features = torch.randn(3, 5, device=dev, requires_grad=True)\n",
        "\n",
        "# Runs and prints the module\n",
        "output = fc(features)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23Epn0HHR_Nq"
      },
      "source": [
        "Autograd is the system PyTorch uses to populate the gradients of weights in a neural network. See [here](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py) for details about PyTorch's autograd. When a module is run on a TPU core, its gradients are also populated on the same TPU core by autograd. The following cell demonstrates this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qs-2q5AMRixo",
        "outputId": "5736a5ee-7b3f-4955-b8df-4375391e168d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5913, 0.1172, 0.3398, 0.9641, 0.6719],\n",
            "        [0.5913, 0.1172, 0.3398, 0.9641, 0.6719]], device='xla:0')\n"
          ]
        }
      ],
      "source": [
        "output.backward(torch.ones_like(output))\n",
        "print(fc.weight.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0g3o1wHmF38"
      },
      "source": [
        "## Running PyTorch networks on TPUs\n",
        "\n",
        "As mentioned above, PyTorch networks are also modules, and so they're run in the same way. The following cell runs a relatively simple PyTorch network from the [PyTorch tutorial docs](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py) on a TPU core:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLtM_M1imkFQ",
        "outputId": "d089fc50-25ca-468f-b787-c3bdbdff0653"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0735,  0.0874,  0.0451,  0.0057,  0.0659, -0.0753,  0.0047, -0.0587,\n",
            "          0.0899,  0.0225]], device='xla:0', grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Simple example network from\n",
        "# https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        # If the size is a square you can only specify a single number\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "\n",
        "# Places network on the default TPU core\n",
        "net = Net().to(dev)\n",
        "\n",
        "# Creates random input on the default TPU core\n",
        "input = torch.randn(1, 1, 32, 32, device=dev)\n",
        "\n",
        "# Runs network\n",
        "out = net(input)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmySVJYIm88W"
      },
      "source": [
        "As in the previous snippets, running PyTorch on a TPU just requires specifying a TPU core as a device."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "puNFlZkh0LdB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Getting Started with PyTorch on Cloud TPUs",
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}